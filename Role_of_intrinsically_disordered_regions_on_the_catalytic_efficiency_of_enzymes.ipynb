{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKI3/nWfIL3C4T1m4aOKyG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joysmasher/Role-of-intrinsically-disordered-regions-on-the-catalytic-efficiency-of-enzymes/blob/main/Role_of_intrinsically_disordered_regions_on_the_catalytic_efficiency_of_enzymes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Code for extracting enzyme data from EXPLORENZYMES and filtration of enzymes based on the presence of Water in their reactions"
      ],
      "metadata": {
        "id": "GHZX7wLeSSJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import gzip\n",
        "import pymysql.cursors\n",
        "import pymysql\n",
        "\n",
        "\n",
        "url = 'https://www.enzyme-database.org/downloads/enzyme-data.sql.gz'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('./enzyme-data.sql.gz', 'wb').write(r.content)\n",
        "a_file = gzip.open(\"./enzyme-data.sql.gz\", \"rb\")\n",
        "contents = a_file.read()\n",
        "open('./enzyme-data.sql', 'wb').write(contents)\n",
        "\n",
        "\n",
        "host = 'database-test.c5yd7id28q2i.us-east-2.rds.amazonaws.com'\n",
        "user = 'admin'\n",
        "password = 'databasetest'\n",
        "database = 'TESTDB'\n",
        "\n",
        "connection = pymysql.connect(host=host,\n",
        "                             user=user,\n",
        "                             password=password,\n",
        "                             db=database,\n",
        "                             charset='utf8mb4',\n",
        "                             cursorclass=pymysql.cursors.DictCursor)\n",
        "\n",
        "cur = connection.cursor()\n",
        "\n",
        "with connection:\n",
        "\n",
        "    # The following command retieves the enzyme data of those reactions that involve water either as a substrate or a product and have a cross reference in PDB Database. These entries do not involve those reactions that have H2O2 as a substrate or a product\n",
        "    cur.execute('select ec_num,accepted_name,reaction,links from entry where reaction LIKE \"%H2O%\" AND links like \"%PDB%\" and reaction NOT LIKE \"%H2O2%\"')\n",
        "    my_f0 = pd.DataFrame()\n",
        "    for i in cur.fetchall():\n",
        "        li = []\n",
        "        for j in i.items():\n",
        "            d = list(j)\n",
        "            li.append(d)\n",
        "        my = pd.DataFrame(li).T\n",
        "        my_f0 = my_f0.append(my, ignore_index=True)\n",
        "    my_f0.drop_duplicates(keep=False, inplace=True)\n",
        "    my_f0.columns = [\"EC_Number\", \"Accepted_name\", \"Reaction\", \"Links\"]\n",
        "    my_f0.sort_values(my_f0.columns[0],axis=0, inplace=True)\n",
        "    #my_f.to_csv(\"ENZYMEwithH2O.csv\")\n",
        "\n",
        "\n",
        "    cur.execute('select ec_num,accepted_name,reaction,links from entry where reaction LIKE \"%Hydrolysis%\" AND links like \"%PDB%\"')\n",
        "    my_f1 = pd.DataFrame()\n",
        "    for i in cur.fetchall():\n",
        "        li = []\n",
        "        for j in i.items():\n",
        "            d = list(j)\n",
        "            li.append(d)\n",
        "        my = pd.DataFrame(li).T\n",
        "        my_f1 = my_f1.append(my, ignore_index=True)\n",
        "    my_f1.drop_duplicates(keep=False, inplace=True)\n",
        "    my_f1.columns = [\"EC_Number\", \"Accepted_name\", \"Reaction\", \"Links\"]\n",
        "    my_f1.sort_values(my_f1.columns[0], axis=0, inplace=True)\n",
        "    #my_f.to_csv(\"ENZYME_with_Hydrolysis.csv\")\n",
        "\n",
        "\n",
        "    cur.execute('select ec_num,accepted_name,reaction,links from entry where links like \"%PDB%\"')\n",
        "    my_f2 = pd.DataFrame()\n",
        "    for i in cur.fetchall():\n",
        "        li = []\n",
        "        for j in i.items():\n",
        "            d = list(j)\n",
        "            li.append(d)\n",
        "        my = pd.DataFrame(li).T\n",
        "        my_f2 = my_f2.append(my, ignore_index=True)\n",
        "    my_f2.drop_duplicates(keep=False, inplace=True)\n",
        "    my_f2.columns = [\"EC_Number\", \"Accepted_name\", \"Reaction\", \"Links\"]\n",
        "    my_f2.sort_values(my_f2.columns[0], axis=0, inplace=True)\n",
        "    #my_f.to_csv(\"ENZYME_in_PDB.csv\")\n",
        "\n",
        "\n",
        "    cur.execute('select ec_num,accepted_name,reaction,links from entry where accepted_name LIKE \"%dehydrogenase%\" AND links like \"%PDB%\"')\n",
        "    my_f3 = pd.DataFrame()\n",
        "    for i in cur.fetchall():\n",
        "        li = []\n",
        "        for j in i.items():\n",
        "            d = list(j)\n",
        "            li.append(d)\n",
        "        my = pd.DataFrame(li).T\n",
        "        my_f3 = my_f3.append(my, ignore_index=True)\n",
        "    my_f3.drop_duplicates(keep=False, inplace=True)\n",
        "    my_f3.columns = [\"EC_Number\", \"Accepted_name\", \"Reaction\", \"Links\"]\n",
        "    my_f3.sort_values(my_f3.columns[0], axis=0, inplace=True)\n",
        "    #my_f.to_csv(\"ENZYME_with_Dehydrogenase.csv\")\n",
        "\n",
        "\n",
        "    cur.execute('select ec_num,accepted_name,reaction,links from entry where reaction LIKE \"%Exonucleolytic%\" AND links like \"%PDB%\"')\n",
        "    my_f4 = pd.DataFrame()\n",
        "    for i in cur.fetchall():\n",
        "        li = []\n",
        "        for j in i.items():\n",
        "            d = list(j)\n",
        "            li.append(d)\n",
        "        my = pd.DataFrame(li).T\n",
        "        my_f4 = my_f4.append(my, ignore_index=True)\n",
        "    my_f4.drop_duplicates(keep=False, inplace=True)\n",
        "    my_f4.columns = [\"EC_Number\", \"Accepted_name\", \"Reaction\", \"Links\"]\n",
        "    my_f4.sort_values(my_f4.columns[0], axis=0, inplace=True)\n",
        "    #my_f.to_csv(\"Exonucleolytic_ENZYME.csv\")\n",
        "\n",
        "\n",
        "    cur.execute('select ec_num,accepted_name,reaction,links from entry where reaction LIKE \"%Endonucleolytic%\" AND links like \"%PDB%\"')\n",
        "    my_f5 = pd.DataFrame()\n",
        "    for i in cur.fetchall():\n",
        "        li = []\n",
        "        for j in i.items():\n",
        "            d = list(j)\n",
        "            li.append(d)\n",
        "        my = pd.DataFrame(li).T\n",
        "        my_f5 = my_f5.append(my, ignore_index=True)\n",
        "    my_f5.drop_duplicates(keep=False, inplace=True)\n",
        "    my_f5.columns = [\"EC_Number\", \"Accepted_name\", \"Reaction\", \"Links\"]\n",
        "    my_f5.sort_values(my_f5.columns[0], axis=0, inplace=True)\n",
        "    #my_f5.to_csv(\"Endonucleolytic_ENZYME.csv\")\n",
        "\n",
        "\n",
        "\n",
        "    file1 = \"ENZYMEwithH2O.csv\"\n",
        "    file2 = \"ENZYME_with_Hydrolysis.csv\"\n",
        "    file3 = \"ENZYME_with_Dehydrogenase.csv\"\n",
        "    file4 = \"Exonucleolytic_ENZYME.csv\"\n",
        "    file5 = \"Endonucleolytic_ENZYME.csv\"\n",
        "\n",
        "\n",
        "    dataFrame1 = pd.concat(map(pd.read_csv, [file1, file2, file3, file4, file5]), ignore_index=True)\n",
        "    dataFrame1.pop('Unnamed: 0')\n",
        "    dataFrame1.sort_values(dataFrame1.columns[0], axis=0, inplace=True)\n",
        "    dataFrame1.drop_duplicates(keep='first', inplace=True)\n",
        "    print(dataFrame1)\n",
        "    dataFrame.to_csv(\"FinalWaterEnzymes.csv\")\n",
        "\n",
        "\n",
        "    dataFrame2= my_f2[~my_f2.apply(tuple, 1).isin(dataFrame1.apply(tuple, 1))]\n",
        "    dataFrame2.drop(dataFrame2.index[545:705],0,inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "QsFcLZRFSKn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Python script for selecting only those enzymes that have PDB IDs available."
      ],
      "metadata": {
        "id": "dK-1C17OShrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import pandas as pd\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "url = 'http://www.ebi.ac.uk/thornton-srv/databases/pdbsum/data/pdb_EC'\n",
        "filename = wget.download(url)\n",
        "print(filename)\n",
        "\n",
        "file1 = open(\"pdb_EC\")\n",
        "my_list= file1.readlines()\n",
        "file1.close()\n",
        "\n",
        "my_my_df = pd.DataFrame()\n",
        "for line in my_list:\n",
        "    s=line.strip().split()\n",
        "    s=pd.DataFrame(s).T\n",
        "    my_my_df=my_my_df.append(s,ignore_index=True)\n",
        "\n",
        "my_my_df.to_csv(\"enzyme_data.csv\")\n",
        "\n",
        "df = pd.read_csv(r'Directory of enzyme_data.csv')\n",
        "df.rename(columns={\"0\": \"PDB ID\", \"1\": \"EC_Number\"}, inplace=True)\n",
        "df.columns\n",
        "\n",
        "# the pop command removes the unwanted columns from the dataframe\n",
        "df.pop('Unnamed: 0')\n",
        "df.pop('2')\n",
        "df.pop('3')\n",
        "df = df[[\"EC_Number\", \"PDB ID\"]]\n",
        "\n",
        "# removes the Chain number from the string containing the PDB ID.\n",
        "df['PDB ID'] = df['PDB ID'].str[:-1]\n",
        "\n",
        "print(df)\n",
        "\n",
        "df1 = pd.read_csv(r'Directory of FinalWaterEnzymes.csv')\n",
        "print(df1)\n",
        "\n",
        "df2 = pd.merge(df,df1)\n",
        "df2.pop('Unnamed: 0')\n",
        "df2.pop('Links')\n",
        "print(df2)\n",
        "\n",
        "df2.drop_duplicates(keep='first', inplace=True)\n",
        "print(df2)\n",
        "\n",
        "myList = list(df2['PDB ID'])\n",
        "print(myList)\n",
        "df2.to_csv(\"ENZYMEwithH2O_PDB ID.csv\")\n",
        "\n",
        "\n",
        "\n",
        "df3 = pd.read_csv(r'Directory of FinalNonWaterEnzymes.csv')\n",
        "print(df3)\n",
        "\n",
        "df4 = pd.merge(df,df3)\n",
        "df4.pop('Unnamed: 0')\n",
        "df4.pop('Links')\n",
        "print(df4)\n",
        "\n",
        "df4.drop_duplicates(keep='first', inplace=True)\n",
        "\n",
        "print(df4)\n",
        "\n",
        "myList = list(df4['PDB ID'])\n",
        "print(myList)\n",
        "df4.to_csv(\"ENZYMEwith0utH2O_PDB ID.csv\")\n"
      ],
      "metadata": {
        "id": "tt0y-IN4Sh6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Code for filtering best PDB structures based on Resolution, B factor, Missing resudue percentages"
      ],
      "metadata": {
        "id": "eEHFj7RpSiG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from concurrent import futures\n",
        "from Bio.PDB.MMCIF2Dict import MMCIF2Dict\n",
        "\n",
        "title = (\" PDB_id \", \" Mean B_value \", \" Resolution \", \" Missing Residue Percentage \", \" Uniprot ID \")\n",
        "\n",
        "# path: Path = Path(\"D:/Series/cif\") #Path to the folder where files need to be downloaded\n",
        "# files_no_ext = ['.'.join(x.split('.')[:-1]) for x in os.listdir(\"D:/Series/cif\") if os.path.isfile(os.path.join('D:/Series/cif', x))]\n",
        "path: Path = Path(\"Directory containing PDB structures\")  # Path to the folder where files need to be downloaded\n",
        "files_no_ext = ['.'.join(x.split('.')[:-1]) for x in os.listdir(\"Directory containing PDB structures\") if\n",
        "                os.path.isfile(os.path.join('Directory containing PDB structures', x))]\n",
        "\n",
        "\n",
        "def LIST(IDs):\n",
        "    cifid = IDs + \".cif\"\n",
        "    filename: Path = Path(cifid)\n",
        "    ver = path / filename  # Return actual path of the file\n",
        "    mmcif_dict = MMCIF2Dict(ver)\n",
        "    print(f'{IDs} started...')\n",
        "\n",
        "    b_mean_str = resolution_str = uniid_str = \"\"\n",
        "\n",
        "    try:\n",
        "        b_mean = mmcif_dict[\"_refine.B_iso_mean\"][0]  # B-factor\n",
        "        b_isomean = b_mean_str.join(b_mean)\n",
        "        if b_isomean == \"?\":\n",
        "            b_isomean = 0\n",
        "    except KeyError:\n",
        "        b_isomean = 0\n",
        "    except AttributeError:\n",
        "        b_isomean = 0\n",
        "\n",
        "    try:\n",
        "        resolution = mmcif_dict[\"_refine.ls_d_res_high\"][0]  # Resolution-High\n",
        "        res_high = resolution_str.join(resolution)\n",
        "        if res_high == \"?\":\n",
        "            res_high = 0\n",
        "    except KeyError:\n",
        "        res_high = 0\n",
        "    except AttributeError:\n",
        "        res_high = 0\n",
        "\n",
        "    try:\n",
        "        missing_res = len(mmcif_dict['_pdbx_unobs_or_zero_occ_residues.auth_comp_id'])  # Missing Residues\n",
        "    except KeyError:\n",
        "        missing_res = 0\n",
        "    except AttributeError:\n",
        "        missing_res = 0\n",
        "\n",
        "    try:\n",
        "        atomlabel = len(mmcif_dict['_pdbx_poly_seq_scheme.mon_id'])  # Deposited Residue Count\n",
        "    except KeyError:\n",
        "        atomlabel = 0\n",
        "    except AttributeError:\n",
        "        atomlabel = 0\n",
        "\n",
        "    try:\n",
        "        missing_res_per = round(((missing_res / atomlabel) * 100), 2)  # Percent of Missing Residues\n",
        "    except ZeroDivisionError:\n",
        "        missing_res_per = 0\n",
        "\n",
        "    try:\n",
        "        uniid = mmcif_dict['_struct_ref.pdbx_db_accession'][0]  # UniProt ID\n",
        "        uniid = uniid_str.join(uniid)\n",
        "    except KeyError:\n",
        "        uniid = str(\"Uniprot ID not found\")\n",
        "    except AttributeError:\n",
        "        uniid = str(\"Uniprot ID not found\")\n",
        "\n",
        "    print(IDs + \" completed,\")\n",
        "    return IDs, b_isomean, res_high, missing_res_per, uniid\n",
        "\n",
        "\n",
        "# ID_read = ['1lz3','1dyc','6BYK','1dyd','1dyf','1dye','1dyg','1dyh','5jkk', '1al0', '107L', '2y7q', '7r2v', '2r2v']\n",
        "ID_read = files_no_ext\n",
        "# ID = pd.read_csv(\"sublist16.csv\", usecols = ['PDB_ID'], low_memory = True)\n",
        "# ID_read = ID.PDB_ID.to_list()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    with futures.ProcessPoolExecutor() as executor:\n",
        "        results = list(tqdm(executor.map(LIST, ID_read), total=len(ID_read)))\n",
        "\n",
        "    t2 = time.perf_counter()\n",
        "\n",
        "    print(f'Finished in {t2 - t1} seconds')\n",
        "    print('All task is done!')\n",
        "    df = pd.DataFrame(results)  # , columns=title)\n",
        "    df.columns = [\" PDB_ID \", \" Mean B_value \", \" Resolution \", \" Missing Residue Percentage \", \" Uniprot ID \"]\n",
        "    df.sort_values(df.columns[0], axis=0, inplace=True)\n",
        "    print(df)\n",
        "    df.to_csv(\"Directory to store Non_Water_Enzymes_main_values.csv\")"
      ],
      "metadata": {
        "id": "BXPvZbofSiKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Code to download Alphafold Structures using Uniprot IDs"
      ],
      "metadata": {
        "id": "PkOUeaBJT2k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "desired_width = 1000\n",
        "pd.set_option('display.width', desired_width)\n",
        "pd.set_option('display.max_columns', 30)\n",
        "pd.set_option('display.max_rows', 30)\n",
        "\n",
        "df = pd.read_csv(r'D:\\output\\Project Output\\CSV Results\\Water Enzymes\\Non_Water_Enzymes_main_values.csv')\n",
        "print(df)\n",
        "df.rename(columns={\" Mean B_value \": \"B_Factor\", \" Resolution \": \"RESOLUTION\", \" PDB_ID \": \"PDB_ID\", \" Missing Residue Percentage \": \"Missing_Residue_Percentage\", \" Uniprot ID \": \"Uniprot ID\"}, inplace=True)\n",
        "print(df)\n",
        "df[\"PDB_ID\"] = df[\"PDB_ID\"].str[:-8]\n",
        "#df.pop('Unnamed: 0')  #this is only for Non Water Enzymes\n",
        "print(df)\n",
        "\n",
        "df.drop_duplicates(subset=[\"Uniprot ID\"], keep='first', inplace=True)\n",
        "print(\"Redundant IDs Removed List\")\n",
        "print(df)\n",
        "#\n",
        "myList = list(df['Uniprot ID'])\n",
        "print(myList)\n",
        "\n",
        "for i in myList:\n",
        "    try:\n",
        "        url = f'https://alphafold.ebi.ac.uk/files/AF-{i}-F1-model_v4.cif'\n",
        "        path1 = 'D:/Output/Enzymes/Alpha Fold Non Water Enzymes/'\n",
        "        filename = wget.download(url, out=path1)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M3Mo2xuPT2oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Match Disordered regions in enzyme sequences"
      ],
      "metadata": {
        "id": "1q3_A-VVNEHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import multiprocessing as mp\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Load the datasets\n",
        "df_non_water = pd.read_csv('/content/Non_Water_Enzymes.csv')\n",
        "df_water = pd.read_csv('/content/Water_Enzymes.csv')\n",
        "df_multireaction = pd.read_csv('/content/Multireaction_Enzymes.csv')\n",
        "\n",
        "# Combine the dataframes\n",
        "combined_df = pd.concat([df_non_water, df_water, df_multireaction], ignore_index=True)\n",
        "\n",
        "# Load the IDR sequences from the FASTA file\n",
        "idr_sequences = []\n",
        "idr_metadata = []\n",
        "for record in SeqIO.parse('/content/DisProt release_2024_06 with_ambiguous_evidences.fasta', 'fasta'):  # Replace with actual path\n",
        "    idr_sequences.append(str(record.seq))\n",
        "    idr_metadata.append(record.description)\n",
        "\n",
        "# Function to validate exact matches of IDRs in enzyme sequences\n",
        "def validate_idr_match(args):\n",
        "    enzyme_seq, idr_seq, idr_details, category, alphafold_id = args\n",
        "    matches = []\n",
        "\n",
        "    # Use regular expression to find all non-overlapping matches of the IDR in the enzyme sequence\n",
        "    match_positions = [m.start() + 1 for m in re.finditer(f'(?={idr_seq})', enzyme_seq)]\n",
        "    actual_count = len(match_positions)\n",
        "    expected_count = enzyme_seq.count(idr_seq)  # Count using built-in string count\n",
        "\n",
        "    # Check if actual match count aligns with the expected count\n",
        "    is_valid = actual_count == expected_count\n",
        "\n",
        "    if actual_count > 0:\n",
        "        matches.append({\n",
        "            'enzyme_sequence': enzyme_seq,\n",
        "            'idr_sequence': idr_seq,\n",
        "            'idr_details': idr_details,\n",
        "            'category': category,\n",
        "            'alphafold_id': alphafold_id,\n",
        "            'expected_count': expected_count,\n",
        "            'actual_count': actual_count,\n",
        "            'positions_found': match_positions,\n",
        "            'is_valid': is_valid\n",
        "        })\n",
        "\n",
        "    return matches\n",
        "\n",
        "# Prepare tasks for multiprocessing\n",
        "tasks = []\n",
        "for _, row in combined_df.iterrows():\n",
        "    enzyme_seq = row['Sequence']\n",
        "    alphafold_id = row['AlphaFold_ID']\n",
        "    category = row['Type']\n",
        "    for idr_seq, idr_details in zip(idr_sequences, idr_metadata):\n",
        "        tasks.append((enzyme_seq, idr_seq, idr_details, category, alphafold_id))\n",
        "\n",
        "# Run the validation using multiprocessing\n",
        "with mp.Pool(mp.cpu_count()) as pool:\n",
        "    results = pool.map(validate_idr_match, tasks)\n",
        "\n",
        "# Flatten the list of lists and filter out empty results\n",
        "flat_results = [item for sublist in results for item in sublist if item]\n",
        "\n",
        "# Convert to DataFrame for further analysis\n",
        "result_df = pd.DataFrame(flat_results)\n",
        "\n",
        "# Save results for each category in each dataset\n",
        "for category in result_df['category'].unique():\n",
        "    category_df = result_df[result_df['category'] == category]\n",
        "    category_filename = f'validated_idr_mapping_{category.replace(\" \", \"_\")}.csv'\n",
        "    category_df.to_csv(category_filename, index=False)\n",
        "    print(f\"Validation results saved for category '{category}' in '{category_filename}'\")\n",
        "\n",
        "# Display a sample for verification\n",
        "print(result_df.head())\n"
      ],
      "metadata": {
        "id": "zDjj2E9o97YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Filter enzyme sequences based on smallest enzyme sequence in each dataset"
      ],
      "metadata": {
        "id": "XKjJbU4f-Nhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the datasets\n",
        "datasets = {\n",
        "    \"Non_Water\": pd.read_csv(\"validated_idr_mapping_Non_Water.csv\"),\n",
        "    \"Non_Water_Multireactions\": pd.read_csv(\"validated_idr_mapping_Non_Water_-_Multireactions.csv\"),\n",
        "    \"Water\": pd.read_csv(\"validated_idr_mapping_Water.csv\"),\n",
        "    \"Water_Multireactions\": pd.read_csv(\"validated_idr_mapping_Water_-_Multireactions.csv\"),\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store the smallest enzyme sequence length for each dataset\n",
        "smallest_lengths = {}\n",
        "\n",
        "# Step 1: Find the smallest enzyme sequence length in each dataset\n",
        "for name, df in datasets.items():\n",
        "    smallest_length = df['enzyme_sequence_length'].min()\n",
        "    smallest_lengths[name] = smallest_length\n",
        "    print(f\"Smallest enzyme sequence length in {name}: {smallest_length}\")\n",
        "\n",
        "# Step 2: Filter each dataset based on the smallest enzyme sequence length\n",
        "filtered_datasets = {}\n",
        "for name, df in datasets.items():\n",
        "    smallest_length = smallest_lengths[name]\n",
        "\n",
        "    # Keep only rows where IDR length is less than or equal to the smallest enzyme sequence length\n",
        "    filtered_df = df[df['idr_sequence_length'] <= smallest_length]\n",
        "\n",
        "    # Save the filtered DataFrame to a new CSV file\n",
        "    filtered_df.to_csv(f\"filtered_{name}.csv\", index=False)\n",
        "\n",
        "    # Store the filtered DataFrame in the dictionary\n",
        "    filtered_datasets[name] = filtered_df\n",
        "    print(f\"Filtered {name} dataset saved with entries containing IDRs <= {smallest_length} amino acids.\")\n",
        "\n",
        "# Output confirmation\n",
        "print(\"All datasets have been filtered and saved based on the smallest enzyme sequence length.\")\n"
      ],
      "metadata": {
        "id": "efI0ST04-OKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Remove Redundancies"
      ],
      "metadata": {
        "id": "xT8lYjSQ-wCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def remove_redundancies(df):\n",
        "    # Drop duplicates based on essential columns excluding metadata\n",
        "    deduplicated_df = df.drop_duplicates(\n",
        "        subset=[\n",
        "            'enzyme_sequence',\n",
        "            'enzyme_sequence_length',\n",
        "            'idr_sequence',\n",
        "            'idr_sequence_length',\n",
        "            'positions_found',\n",
        "            'start_positions',\n",
        "            'end_positions',\n",
        "            'is_valid'\n",
        "        ]\n",
        "    )\n",
        "    return deduplicated_df\n",
        "\n",
        "# Apply the redundancy removal across all datasets\n",
        "categories = ['Non_Water', 'Water', 'Non_Water_Multireactions', 'Water_Multireactions']\n",
        "filtered_data = {}\n",
        "\n",
        "for category in categories:\n",
        "    filename = f'filtered_{category}.csv'\n",
        "    df = pd.read_csv(filename)\n",
        "    filtered_data[category] = remove_redundancies(df)\n",
        "\n",
        "# Optionally save the cleaned datasets\n",
        "for category, df in filtered_data.items():\n",
        "    df.to_csv(f'Clean_filtered_{category}.csv', index=False)\n"
      ],
      "metadata": {
        "id": "dobtFKzF-wFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## IDR length count"
      ],
      "metadata": {
        "id": "Ss3c1UDdAZJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the filtered datasets\n",
        "categories = ['Non_Water', 'Water', 'Non_Water_Multireactions', 'Water_Multireactions']\n",
        "filtered_data = {}\n",
        "\n",
        "for category in categories:\n",
        "    filename = f'Clean_filtered_{category}.csv'\n",
        "    df = pd.read_csv(filename)\n",
        "    df['dataset'] = category  # Add a dataset column to distinguish categories\n",
        "    filtered_data[category] = df\n",
        "\n",
        "# Step 2: Prepare dataframes containing IDR lengths and their counts for each category\n",
        "idr_length_data = []\n",
        "\n",
        "for category, df in filtered_data.items():\n",
        "    # Group by IDR length and count occurrences\n",
        "    idr_length_counts = df.groupby('idr_sequence_length').size().reset_index(name='count')\n",
        "    idr_length_counts['dataset'] = category  # Add the dataset name\n",
        "    idr_length_data.append(idr_length_counts)\n",
        "\n",
        "# Combine all datasets into one DataFrame\n",
        "combined_idr_length_data = pd.concat(idr_length_data, ignore_index=True)\n",
        "\n",
        "# Display the combined dataframe for interpretation\n",
        "print(\"Combined IDR Length Data:\")\n",
        "print(combined_idr_length_data)\n"
      ],
      "metadata": {
        "id": "PDIhQFa_AZNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Similar approach applied to generate IDR data of Nonenzymatic Proteins##"
      ],
      "metadata": {
        "id": "_FggUtGzBwI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The below given code fetches the active site residue information and measure the average distance between IDRs and active sites in enzyme categories"
      ],
      "metadata": {
        "id": "9AaeLdmiDXBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xml.dom.minidom\n",
        "import wget\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import itertools\n",
        "from scipy.stats import ttest_ind\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "# Load the filtered datasets\n",
        "categories = ['Non_Water', 'Water',\n",
        "              'Non_Water_Multireactions', 'Water_Multireactions']\n",
        "filtered_data = {}\n",
        "\n",
        "for category in categories:\n",
        "    filename = f'Clean_filtered_{category}.csv'\n",
        "    filtered_data[category] = pd.read_csv(filename)\n",
        "\n",
        "# Define a consistent color palette\n",
        "palette = sns.color_palette('muted', len(categories))\n",
        "category_colors = {category: palette[i] for i, category in enumerate(categories)}\n",
        "\n",
        "# Function to extract UniProt ID from AlphaFold ID\n",
        "def extract_uniprot_id(df):\n",
        "    df['Uniprot_ID'] = df['AlphaFold_ID'].str.extract(r'AF-([A-Z0-9]+)-')[0]\n",
        "    return df\n",
        "\n",
        "# Apply the function to extract UniProt ID for each dataset\n",
        "for category, df in filtered_data.items():\n",
        "    filtered_data[category] = extract_uniprot_id(df)\n",
        "\n",
        "# Function to get active site positions for each UniProt ID\n",
        "def get_active_sites(uniprot_id):\n",
        "    url = f'https://rest.uniprot.org/uniprotkb/{uniprot_id}.xml'\n",
        "    try:\n",
        "        filename = wget.download(url)\n",
        "        domtree = xml.dom.minidom.parse(filename)\n",
        "        group = domtree.documentElement\n",
        "        active_sites = []\n",
        "\n",
        "        # Extract active site positions\n",
        "        for feature in group.getElementsByTagName('feature'):\n",
        "            if feature.getAttribute(\"type\") == \"active site\":\n",
        "                position = feature.getElementsByTagName('position')\n",
        "                if position:\n",
        "                    active_sites.append(int(position[0].getAttribute(\"position\")))\n",
        "\n",
        "        # Delete the downloaded file\n",
        "        os.remove(filename)\n",
        "\n",
        "        return active_sites if active_sites else None\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {uniprot_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to process a single row and calculate distances\n",
        "def process_row(row):\n",
        "    active_sites = get_active_sites(row['Uniprot_ID'])\n",
        "    if active_sites is not None:\n",
        "        idr_positions = list(zip(eval(row['start_positions']), eval(row['end_positions'])))\n",
        "        distances = []\n",
        "\n",
        "        for active_site in active_sites:\n",
        "            # Calculate the distance from each IDR to the active site\n",
        "            idr_distances = [\n",
        "                min(abs(active_site - start), abs(active_site - end))\n",
        "                for start, end in idr_positions\n",
        "            ]\n",
        "            distances.append(np.mean(idr_distances))  # Average distance to this active site\n",
        "\n",
        "        # Average distance for all active sites in this sequence\n",
        "        return active_sites, np.mean(distances)\n",
        "    else:\n",
        "        return None, np.nan  # No active site found\n",
        "\n",
        "# Parallelized processing function\n",
        "def process_dataframe(df):\n",
        "    rows = df.to_dict('records')\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        results = pool.map(process_row, rows)\n",
        "\n",
        "    # Unpack results\n",
        "    active_site_positions, avg_distances = zip(*results)\n",
        "    df['Active_Site_Positions'] = active_site_positions\n",
        "    df['Average_Distance_to_Active_Site'] = avg_distances\n",
        "\n",
        "    return df\n",
        "\n",
        "# Process each dataset in parallel\n",
        "processed_data = {}\n",
        "for category, df in filtered_data.items():\n",
        "    print(f\"Processing category: {category}\")\n",
        "    processed_data[category] = process_dataframe(df)\n",
        "\n",
        "# Save processed datasets\n",
        "for category, df in processed_data.items():\n",
        "    df.to_csv(f'Processed_Cleaned_{category}_with_active_site_distances.csv', index=False)\n",
        "\n",
        "# Collect distances for statistical analysis\n",
        "distance_data = {}\n",
        "for category, df in processed_data.items():\n",
        "    # Exclude NaN values\n",
        "    distances = df['Average_Distance_to_Active_Site'].dropna().astype(float)\n",
        "    distance_data[category] = distances\n",
        "\n",
        "# Calculate descriptive statistics for each category\n",
        "stats_results = []\n",
        "for category, distances in distance_data.items():\n",
        "    mean_distance = distances.mean()\n",
        "    std_distance = distances.std()\n",
        "    sem_distance = std_distance / np.sqrt(len(distances))\n",
        "\n",
        "    stats_results.append({\n",
        "        'Category': category,\n",
        "        'Mean_Distance': mean_distance,\n",
        "        'Std_Distance': std_distance,\n",
        "        'SEM_Distance': sem_distance,\n",
        "        'Count': len(distances)\n",
        "    })\n",
        "\n",
        "# Convert statistics results to DataFrame\n",
        "stats_df = pd.DataFrame(stats_results)\n",
        "\n",
        "# Plot average distances with error bars\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    data=stats_df,\n",
        "    x='Category',\n",
        "    y='Mean_Distance',\n",
        "    palette=[category_colors[cat] for cat in stats_df['Category']],\n",
        "    edgecolor='black'\n",
        ")\n",
        "plt.errorbar(\n",
        "    stats_df['Category'],\n",
        "    stats_df['Mean_Distance'],\n",
        "    yerr=stats_df['SEM_Distance'],\n",
        "    fmt='none',\n",
        "    capsize=5,\n",
        "    color='black'\n",
        ")\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel(\"Category\", fontsize=12)\n",
        "plt.ylabel(\"Mean Distance to Active Site\", fontsize=12)\n",
        "plt.title(\"Mean Distance to Active Sites with Standard Error\", fontsize=14)\n",
        "plt.xticks(rotation=45, fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot\n",
        "plt.savefig('Mean_Distance_to_Active_Sites.png', dpi=900)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VhMq13qCKRj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The below is an example code for extracting the 3D coordinates of active sites from the Alphafold Structures and measuring centroid distances"
      ],
      "metadata": {
        "id": "jy_16fy2P5Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.dom.minidom\n",
        "import pandas as pd\n",
        "from Bio.PDB.MMCIF2Dict import MMCIF2Dict\n",
        "import numpy as np\n",
        "import sys\n",
        "import wget\n",
        "from Bio.PDB import *\n",
        "import math\n",
        "from biopandas.mmcif import PandasMmcif\n",
        "from Bio.PDB.ResidueDepth import *\n",
        "from biopandas.pdb import PandasPdb\n",
        "\n",
        "\n",
        "desired_width = 1000\n",
        "pd.set_option('display.width', desired_width)\n",
        "pd.set_option('display.max_columns', 30)\n",
        "\n",
        "\n",
        "def find_indices(list_to_check, item_to_find):\n",
        "    return[idx for idx, value in enumerate(list_to_check) if value == item_to_find]\n",
        "\n",
        "\n",
        "df = pd.read_csv(r'Enter the path where the enzyme csv file is located')\n",
        "print(df)\n",
        "myList = list(df['Uniprot ID'])\n",
        "myList1 = list(df['AlphaFold_ID'])\n",
        "# print(myList)\n",
        "# print(myList1)\n",
        "\n",
        "# sys.exit()\n",
        "A_loc = []\n",
        "m_res={}\n",
        "# for j in range(len(myList)):\n",
        "for j in range(20):\n",
        "    try:\n",
        "        url = f'https://rest.uniprot.org/uniprotkb/{myList[j]}.xml'\n",
        "        filename = wget.download(url)\n",
        "        domtree = xml.dom.minidom.parse(filename)\n",
        "        group = domtree.documentElement\n",
        "        # people = group.getElementsByTagName('subcellularLocation')\n",
        "        active = group.getElementsByTagName('feature')\n",
        "\n",
        "### Extract the subcellular location details of Enzymes from XML file\n",
        "        # for person in people:\n",
        "        #     loc = person.getElementsByTagName('location')[0].childNodes[0].nodeValue\n",
        "        # print(j, \"   Subcellular Location:\", loc)\n",
        "### Store the Uniprot ID and its Cellular location in a list\n",
        "        # data_1 = myList[j], loc\n",
        "        # A_loc.append(data_1)\n",
        "        #sys.exit()\n",
        "### Extract the active site residue positions from XML file\n",
        "        txt = []\n",
        "        for site in active:\n",
        "            res = []\n",
        "            if site.getAttribute(\"type\") == \"active site\":\n",
        "                act = site.getElementsByTagName('position')[0].getAttributeNode('position').nodeValue\n",
        "                txt.append(act)\n",
        "                res = [ele for ele in txt if\n",
        "                       txt != []]  ## Creates a new list from the previous appended list containing only non empty elements  ##\n",
        "                m_res[myList1[j]] = res\n",
        "        # print(myList[j], myList1[j], \"   Active Sites:\", res)\n",
        "        print(m_res)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "## Store the list containing subcellular location details in a dataframe and sace as a CSV file\n",
        "# Subcellular = pd.DataFrame(A_loc,columns=[\"Uniprot ID\", \"Subcellular Location\"])\n",
        "# print(Subcellular)\n",
        "# Subcellular.to_csv(\"D:/output/Enzymes/AlphaFold CSV Results/Water/Subcellular.csv\")\n",
        "#\n",
        "\n",
        "### Delete the downloaded XML files from the directory ###\n",
        "dir_name = \"Directory where the xml file is downloaded\"\n",
        "test = os.listdir(dir_name)\n",
        "for item in test:\n",
        "    if item.endswith(\".xml\"):\n",
        "        os.remove(os.path.join(dir_name, item))\n",
        "\n",
        "print(m_res)\n",
        "# sys.exit()\n",
        "Li = []\n",
        "A_New_List = []\n",
        "for y,z in m_res.items():\n",
        "    print(y,z)\n",
        "    mmcif_dict = MMCIF2Dict.MMCIF2Dict(f'Path where the Alphafold structures are stored{y}')\n",
        "    LIST = []\n",
        "    for m in z:\n",
        "        if m in mmcif_dict['_atom_site.auth_seq_id']:\n",
        "            lost = mmcif_dict['_atom_site.auth_seq_id']\n",
        "            ins = find_indices(lost, m)\n",
        "            print(type(ins))\n",
        "            print(ins[1])\n",
        "            LIST.append(ins[5])\n",
        "            print(LIST)\n",
        "            # b = [(idx, item) for idx,item in enumerate(ins)]\n",
        "            # print(b[1])\n",
        "\n",
        "            for i in LIST:\n",
        "                if i != 0:\n",
        "                    # print(mmcif_dict['_atom_site.Cartn_x'][i], mmcif_dict['_atom_site.Cartn_y'][i], mmcif_dict['_atom_site.Cartn_z'][i])\n",
        "                    Data = float(mmcif_dict['_atom_site.Cartn_x'][i]), float(mmcif_dict['_atom_site.Cartn_y'][i]), float(\n",
        "                        mmcif_dict['_atom_site.Cartn_z'][i])\n",
        "                    Li.append(Data)\n",
        "    print('Appended List is: \\n', Li)\n",
        "    x_sum = 0\n",
        "    y_sum = 0\n",
        "    z_sum = 0\n",
        "\n",
        "    try:\n",
        "        for i in Li:\n",
        "            x_sum += i[0]\n",
        "            y_sum += i[1]\n",
        "            z_sum += i[2]\n",
        "\n",
        "        x_mean = x_sum / len(Li)\n",
        "        y_mean = y_sum / len(Li)\n",
        "        z_mean = z_sum / len(Li)\n",
        "\n",
        "        centroid = x_mean, y_mean, z_mean\n",
        "        Centroid_Active = np.asarray(centroid)\n",
        "\n",
        "        InFile = f'{y}'\n",
        "        structure = MMCIFParser(QUIET=True).get_structure(InFile,\n",
        "                                                          f'Path where Alphafold structures are stored{y}')\n",
        "\n",
        "        pmmcif = PandasMmcif()  #####\n",
        "        pmmcif = pmmcif.read_mmcif(f'Path where Alphafold structures are stored{y}')  #####\n",
        "\n",
        "        ppd3 = pmmcif.df['ATOM'][pmmcif.df['ATOM']['label_atom_id'] == 'CA']\n",
        "        ppd4 = pmmcif.df['ATOM'][pmmcif.df['ATOM']['label_atom_id'] == 'CB']\n",
        "        # r = PandasPdb.rmsd(pmmcif.df['ATOM'][pmmcif.df['ATOM']['label_atom_id'] == 'CA'], pmmcif.df['ATOM'][pmmcif.df['ATOM']['label_atom_id'] == 'CB'])\n",
        "        # print('RMSD: %.4f Angstrom' % r)\n",
        "        # print(ppd2[['x_coord', 'y_coord', 'z_coord']])\n",
        "        add_x = 0\n",
        "        for a in ppd3['Cartn_x'].keys():\n",
        "            add_x = add_x + ppd3['Cartn_x'][a]\n",
        "        CmX = (12.0107 * add_x) / (len(ppd3['Cartn_x']) * 12.0107)\n",
        "\n",
        "        add_y = 0\n",
        "        for b in ppd3['Cartn_y'].keys():\n",
        "            add_y = add_y + ppd3['Cartn_y'][b]\n",
        "        CmY = ((12.0107 * add_y) / (len(ppd3['Cartn_y']) * 12.0107))\n",
        "\n",
        "        add_z = 0\n",
        "        for c in ppd3['Cartn_z'].keys():\n",
        "            add_z = add_z + ppd3['Cartn_z'][c]\n",
        "        CmZ = ((12.0107 * add_z) / (len(ppd3['Cartn_z']) * 12.0107))\n",
        "\n",
        "        Centroid = CmX, CmY, CmZ\n",
        "        Centroid_CA = np.asarray(Centroid)\n",
        "\n",
        "############################################################################################################################\n",
        "\n",
        "\n",
        "        Centroid_Distance = math.dist(structure.center_of_mass(geometric=True), Centroid_Active)\n",
        "        Ca_Centroid_Distance = math.dist(Centroid_CA, Centroid_Active)\n",
        "\n",
        "        print(\"Centroid Distance for\", y, \"is\", Centroid_Distance)\n",
        "        print('Ca Centroid Distance for', y, \"is\", Ca_Centroid_Distance)\n",
        "\n",
        "        data = y, Centroid_Active, round(Centroid_Distance, 2), round(Ca_Centroid_Distance, 2)\n",
        "        A_New_List.append(data)\n",
        "    except ZeroDivisionError:\n",
        "        print(y, \"Has no active sites\")\n",
        "\n",
        "print(A_New_List)\n",
        "Active_Site_Centroid_Water = pd.DataFrame(A_New_List,\n",
        "                                          columns=[\"AlphaFold_ID\", \"Active site Centroid\", \"Prot-Active Centroid Dist\",\n",
        "                                                   \"Ca Prot-Act Distance\"])\n",
        "Active_Site_Centroid_Water[\"AlphaFold_ID\"] = Active_Site_Centroid_Water[\"AlphaFold_ID\"].str[:-4]\n",
        "print(Active_Site_Centroid_Water)\n",
        "Active_Site_Centroid_Water.to_csv(\"Directory where you wish to save the result CSV file\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jiSamiw-P5WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mW7uoLZ_R10y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot for IDR Prevalence based on IDR size"
      ],
      "metadata": {
        "id": "XSSVFuGACzsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/idr_lengths_combined.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Calculate counts for idr_sequence_length for each category\n",
        "data['idr_sequence_length'] = data['idr_sequence_length'].astype(int)\n",
        "\n",
        "# Update to use the 'dataset' column as categories\n",
        "categories = data['dataset'].unique()\n",
        "\n",
        "# Initialize an empty dictionary to store percentages\n",
        "percentage_data = {}\n",
        "\n",
        "# Calculate percentages for each category\n",
        "for category in categories:\n",
        "    category_data = data[data['dataset'] == category]\n",
        "    counts = category_data.groupby('idr_sequence_length')['count'].sum().sort_index()\n",
        "    percentages = (counts / counts.sum()) * 100\n",
        "    percentage_data[category] = percentages\n",
        "\n",
        "# Convert the dictionary to a DataFrame for easier plotting\n",
        "percentage_df = pd.DataFrame(percentage_data).fillna(0)\n",
        "\n",
        "\n",
        "# Define a consistent color palette for the datasets\n",
        "categories = ['Enzymatic_Proteins', 'Nonenzymatic_Proteins']\n",
        "palette = sns.color_palette('muted', len(categories))\n",
        "category_colors = {category: palette[i] for i, category in enumerate(categories)}\n",
        "\n",
        "# Plot the histogram\n",
        "plt.figure(figsize=(12, 8))\n",
        "for category in categories:\n",
        "    plt.plot(\n",
        "        percentage_df.index,\n",
        "        percentage_df[category],\n",
        "        label=category,\n",
        "        color=category_colors[category]  # Use consistent colors\n",
        "    )\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Comparative Histogram of IDR Sequence Lengths by Dataset', fontsize=16)\n",
        "plt.xlabel('IDR Sequence Length', fontsize=14)\n",
        "plt.ylabel('IDR Percentage (%)', fontsize=14)\n",
        "plt.xticks(range(0, 101, 10), fontsize=12)\n",
        "plt.yticks(range(0, 101, 10), fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.8)\n",
        "plt.legend(title='Dataset', fontsize=12, title_fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('Comparative_Histogram_IDR_Sequence_Lengths.png', dpi=300)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "EkNTMptYBwTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## IDR mapping aross Enzymatic and Nonenzymatic proteins"
      ],
      "metadata": {
        "id": "nnBMSDpXBwXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Load the filtered datasets\n",
        "categories = ['Nonenzymatic_Proteins', 'Enzymatic_Proteins']\n",
        "filtered_data = {}\n",
        "\n",
        "for category in categories:\n",
        "    filename = f'Clean_filtered_{category}.csv'\n",
        "    filtered_data[category] = pd.read_csv(filename)\n",
        "\n",
        "# Define consistent color palette for categories\n",
        "import seaborn as sns\n",
        "palette = sns.color_palette('muted', len(categories))\n",
        "category_colors = {category: palette[i] for i, category in enumerate(categories)}\n",
        "\n",
        "# Define colors for IDR lengths using Viridis colormap\n",
        "idr_colors = plt.cm.viridis\n",
        "\n",
        "# Normalize the enzyme sequence lengths and map IDRs based on their length\n",
        "def plot_idrs_by_length_normalized(df, category):\n",
        "    # Ensure start and end positions match correctly\n",
        "    df['start_positions'] = df['positions_found'].apply(lambda x: eval(x))  # Convert list-like strings to lists\n",
        "    df['end_positions'] = df.apply(\n",
        "        lambda row: [start + row['idr_sequence_length'] for start in row['start_positions']],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Set up figure\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Loop through each IDR length\n",
        "    for idr_length in sorted(df['idr_sequence_length'].unique()):\n",
        "        # Filter IDRs by the current length\n",
        "        idr_df = df[df['idr_sequence_length'] == idr_length]\n",
        "\n",
        "        # Loop through each enzyme sequence\n",
        "        for enzyme_sequence, group in idr_df.groupby('enzyme_sequence'):\n",
        "            normalized_length = group['enzyme_sequence_length'].iloc[0]\n",
        "\n",
        "            # Plot each IDR for this enzyme sequence\n",
        "            for _, row in group.iterrows():\n",
        "                for start, end in zip(row['start_positions'], row['end_positions']):\n",
        "                    start_normalized = start / normalized_length\n",
        "                    end_normalized = end / normalized_length\n",
        "                    rect = patches.Rectangle(\n",
        "                        (start_normalized, idr_length - 0.2),\n",
        "                        end_normalized - start_normalized,\n",
        "                        0.4,\n",
        "                        color=idr_colors(idr_length / max(df['idr_sequence_length'])),\n",
        "                        alpha=0.6\n",
        "                    )\n",
        "                    ax.add_patch(rect)\n",
        "\n",
        "    # Configure plot\n",
        "    ax.set_ylim(1, max(df['idr_sequence_length']) + 1)\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_yticks(sorted(df['idr_sequence_length'].unique()))\n",
        "    ax.set_yticklabels(sorted(df['idr_sequence_length'].unique()))\n",
        "    ax.set_xlabel(\"Normalized Sequence Length\")\n",
        "    ax.set_ylabel(\"IDR Length\")\n",
        "    ax.set_title(f\"IDR Length Distribution Across Normalized Enzyme Sequences - {category}\")\n",
        "\n",
        "    # Add color bar\n",
        "    sm = plt.cm.ScalarMappable(cmap=idr_colors, norm=plt.Normalize(vmin=1, vmax=max(df['idr_sequence_length'])))\n",
        "    sm.set_array([])\n",
        "    plt.colorbar(sm, ax=ax, label=\"IDR Length\")\n",
        "\n",
        "    # Save the plot\n",
        "    plt.tight_layout()\n",
        "    # plt.savefig(f'IDR_Length_Distribution_{category}.png', dpi=900)\n",
        "    plt.show()\n",
        "\n",
        "# Generate plots for each dataset\n",
        "for category, df in filtered_data.items():\n",
        "    plot_idrs_by_length_normalized(df, category)\n"
      ],
      "metadata": {
        "id": "5IbH1SWeDx-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Computational prediction of IDRs using RIDAO web server.\n",
        "# Multiple sequences are submitted in RIDAO in fasta format. prediction results are obtained in .tab format as mentioned here as \"result_enzyme.tab\" & \"results_protein.tab\".\n",
        "# The below code plots the dCDF, dCH and PER(MDP) values and performs statistical tests on them."
      ],
      "metadata": {
        "id": "7CWDUeXNSe-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ttest_ind, mannwhitneyu\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for clean output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Step 1: Load and clean the data\n",
        "with open(\"/content/results_enzyme.tab\", \"r\") as f:\n",
        "    enzyme_header = f.readline().strip().split(\",\")\n",
        "enzyme_data = pd.read_csv(\"/content/results_enzyme.tab\", sep=\",\", skiprows=1, header=None)\n",
        "enzyme_data.columns = [col.strip() for col in enzyme_header]\n",
        "enzyme_data[\"Type\"] = \"Enzyme\"\n",
        "\n",
        "with open(\"/content/results_protein.tab\", \"r\") as f:\n",
        "    protein_header = f.readline().strip().split(\",\")\n",
        "protein_data = pd.read_csv(\"/content/results_protein.tab\", sep=\",\", skiprows=1, header=None)\n",
        "protein_data.columns = [col.strip() for col in protein_header]\n",
        "protein_data[\"Type\"] = \"Protein\"\n",
        "\n",
        "# Combine datasets\n",
        "combined_df = pd.concat([enzyme_data, protein_data], ignore_index=True)\n",
        "\n",
        "# Convert key columns to numeric\n",
        "for col in [\"dCH\", \"dCDF\", \"PER(MDP)\"]:\n",
        "    combined_df[col] = pd.to_numeric(combined_df[col], errors=\"coerce\")\n",
        "\n",
        "# Split by type\n",
        "enzyme = combined_df[combined_df[\"Type\"] == \"Enzyme\"]\n",
        "protein = combined_df[combined_df[\"Type\"] == \"Protein\"]\n",
        "\n",
        "# Step 2: Generate plots\n",
        "fig, axs = plt.subplots(4, 2, figsize=(16, 16))\n",
        "axs = axs.flatten()\n",
        "color_palette = {\"Enzyme\": \"#1f77b4\", \"Protein\": \"#d62728\"}\n",
        "\n",
        "# A. CH-CDF scatter\n",
        "axs[0].scatter(enzyme[\"dCH\"], enzyme[\"dCDF\"], alpha=0.4, label=\"Enzymatic Proteins\", color=color_palette[\"Enzyme\"])\n",
        "axs[0].scatter(protein[\"dCH\"], protein[\"dCDF\"], alpha=0.4, label=\"Nonenzymatic Proteins\", color=color_palette[\"Protein\"])\n",
        "axs[0].set_xlabel(\"dCH (Charge-Hydropathy Index)\")\n",
        "axs[0].set_ylabel(\"dCDF (Disorder CDF Index)\")\n",
        "axs[0].set_title(\"A. CH-CDF Scatter Plot\")\n",
        "axs[0].legend()\n",
        "\n",
        "# B. Distribution of dCH\n",
        "sns.histplot(enzyme[\"dCH\"], kde=True, stat=\"density\", ax=axs[1], color=color_palette[\"Enzyme\"], label=\"Enzymatic Proteins\", alpha=0.6)\n",
        "sns.histplot(protein[\"dCH\"], kde=True, stat=\"density\", ax=axs[1], color=color_palette[\"Protein\"], label=\"Nonenzymatic Proteins\", alpha=0.6)\n",
        "axs[1].set_title(\"B. Distribution of dCH\")\n",
        "axs[1].legend()\n",
        "\n",
        "# C. Distribution of dCDF\n",
        "sns.histplot(enzyme[\"dCDF\"], kde=True, stat=\"density\", ax=axs[2], color=color_palette[\"Enzyme\"], label=\"Enzymatic Proteins\", alpha=0.6)\n",
        "sns.histplot(protein[\"dCDF\"], kde=True, stat=\"density\", ax=axs[2], color=color_palette[\"Protein\"], label=\"Nonenzymatic Proteins\", alpha=0.6)\n",
        "axs[2].set_title(\"C. Distribution of dCDF\")\n",
        "axs[2].legend()\n",
        "\n",
        "# D. CDF of dCDF\n",
        "sns.ecdfplot(enzyme[\"dCDF\"].dropna(), ax=axs[3], label=\"Enzymatic Proteins\", color=color_palette[\"Enzyme\"])\n",
        "sns.ecdfplot(protein[\"dCDF\"].dropna(), ax=axs[3], label=\"Nonenzymatic Proteins\", color=color_palette[\"Protein\"])\n",
        "axs[3].set_title(\"D. Cumulative Distribution of dCDF\")\n",
        "axs[3].legend()\n",
        "\n",
        "# E. Distribution of PER(MDP)\n",
        "sns.histplot(enzyme[\"PER(MDP)\"], kde=True, stat=\"density\", ax=axs[4], color=color_palette[\"Enzyme\"], label=\"Enzymatic Proteins\", alpha=0.6)\n",
        "sns.histplot(protein[\"PER(MDP)\"], kde=True, stat=\"density\", ax=axs[4], color=color_palette[\"Protein\"], label=\"Nonenzymatic Proteins\", alpha=0.6)\n",
        "axs[4].set_title(\"E. Distribution of PER(MDP)\")\n",
        "axs[4].legend()\n",
        "\n",
        "# F. CDF of dCH\n",
        "sns.ecdfplot(enzyme[\"dCH\"].dropna(), ax=axs[5], label=\"Enzymatic Proteins\", color=color_palette[\"Enzyme\"])\n",
        "sns.ecdfplot(protein[\"dCH\"].dropna(), ax=axs[5], label=\"Nonenzymatic Proteins\", color=color_palette[\"Protein\"])\n",
        "axs[5].set_title(\"F. Cumulative Distribution of dCH\")\n",
        "axs[5].legend()\n",
        "\n",
        "# G. CDF of PER(MDP)\n",
        "sns.ecdfplot(enzyme[\"PER(MDP)\"].dropna(), ax=axs[6], label=\"Enzymatic Proteins\", color=color_palette[\"Enzyme\"])\n",
        "sns.ecdfplot(protein[\"PER(MDP)\"].dropna(), ax=axs[6], label=\"Nonenzymatic Proteins\", color=color_palette[\"Protein\"])\n",
        "axs[6].set_title(\"G. Cumulative Distribution of PER(MDP)\")\n",
        "axs[6].legend()\n",
        "\n",
        "# Hide the unused subplot\n",
        "axs[7].axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "from statsmodels.stats.power import TTestIndPower\n",
        "\n",
        "# Replace Mann-Whitney with Bonferroni-corrected t-test and add Cohen's d and Power\n",
        "results = []\n",
        "metrics = [\"dCH\", \"dCDF\", \"PER(MDP)\"]\n",
        "alpha = 0.05\n",
        "corrected_alpha = alpha / len(metrics)\n",
        "power_analysis = TTestIndPower()\n",
        "\n",
        "for metric in metrics:\n",
        "    e_vals = enzyme[metric].dropna()\n",
        "    p_vals = protein[metric].dropna()\n",
        "\n",
        "    # Welch's t-test\n",
        "    t_stat, t_pval = ttest_ind(e_vals, p_vals, equal_var=False)\n",
        "    bonf_pval = min(t_pval * len(metrics), 1.0)\n",
        "\n",
        "    # Calculate Cohen's d\n",
        "    mean_diff = e_vals.mean() - p_vals.mean()\n",
        "    pooled_sd = np.sqrt(((e_vals.std() ** 2) + (p_vals.std() ** 2)) / 2)\n",
        "    cohen_d = mean_diff / pooled_sd\n",
        "\n",
        "    # Calculate power\n",
        "    n1, n2 = len(e_vals), len(p_vals)\n",
        "    power = power_analysis.solve_power(effect_size=abs(cohen_d), nobs1=n1, ratio=n2/n1, alpha=corrected_alpha, alternative='two-sided')\n",
        "\n",
        "    results.append({\n",
        "        \"Metric\": metric,\n",
        "        \"Mean_Enzyme\": e_vals.mean(),\n",
        "        \"Mean_Protein\": p_vals.mean(),\n",
        "        \"T-test p-value\": t_pval,\n",
        "        \"Bonferroni-corrected p\": bonf_pval,\n",
        "        \"Cohen's d\": cohen_d,\n",
        "        \"Power (1-β)\": power\n",
        "    })\n",
        "\n",
        "summary_df_corrected = pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "at2TGwJISfdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save summary stats (mean, std, sem) to a separate CSV\n",
        "summary_stats = []\n",
        "\n",
        "for metric in metrics:\n",
        "    for group_name, group_data in [(\"Enzyme\", enzyme), (\"Protein\", protein)]:\n",
        "        values = group_data[metric].dropna()\n",
        "        summary_stats.append({\n",
        "            \"Metric\": metric,\n",
        "            \"Group\": group_name,\n",
        "            \"Mean\": values.mean(),\n",
        "            \"Std\": values.std(),\n",
        "            \"SEM\": values.sem()\n",
        "        })\n",
        "\n",
        "summary_stats_df = pd.DataFrame(summary_stats)\n",
        "\n",
        "# Save both files\n",
        "summary_stats_path = \"/content/Disorder_Metric_Summary_Stats_v2.csv\"\n",
        "ttest_results_path = \"/content/Disorder_TTest_Results_v2.csv\"\n",
        "tiff_image_path = \"/content/RIDAO_Disorder_Comparison_v2.tiff\"\n",
        "\n",
        "summary_stats_df.to_csv(summary_stats_path, index=False)\n",
        "summary_df_corrected.to_csv(ttest_results_path, index=False)\n",
        "\n",
        "# Save the plot as TIFF\n",
        "fig, axs = plt.subplots(4, 2, figsize=(16, 16))\n",
        "axs = axs.flatten()\n",
        "\n",
        "# Reuse the existing plotting logic\n",
        "axs[0].scatter(enzyme[\"dCH\"], enzyme[\"dCDF\"], alpha=0.4, label=\"Enzymatic Proteins\", color=color_palette[\"Enzyme\"])\n",
        "axs[0].scatter(protein[\"dCH\"], protein[\"dCDF\"], alpha=0.4, label=\"Nonenzymatic Proteins\", color=color_palette[\"Protein\"])\n",
        "axs[0].set_xlabel(\"dCH (Charge-Hydropathy Index)\")\n",
        "axs[0].set_ylabel(\"dCDF (Disorder CDF Index)\")\n",
        "axs[0].set_title(\"A. CH-CDF Scatter Plot\")\n",
        "axs[0].legend()\n",
        "\n",
        "sns.histplot(enzyme[\"dCH\"], kde=True, stat=\"density\", ax=axs[1], color=color_palette[\"Enzyme\"], label=\"Enzymatic Proteins\", alpha=0.6)\n",
        "sns.histplot(protein[\"dCH\"], kde=True, stat=\"density\", ax=axs[1], color=color_palette[\"Protein\"], label=\"Nonenzymatic Proteins\", alpha=0.6)\n",
        "axs[1].set_title(\"B. Distribution of dCH\")\n",
        "axs[1].legend()\n",
        "\n",
        "sns.histplot(enzyme[\"dCDF\"], kde=True, stat=\"density\", ax=axs[2], color=color_palette[\"Enzyme\"], label=\"Enzymatic Proteins\", alpha=0.6)\n",
        "sns.histplot(protein[\"dCDF\"], kde=True, stat=\"density\", ax=axs[2], color=color_palette[\"Protein\"], label=\"Nonenzymatic Proteins\", alpha=0.6)\n",
        "axs[2].set_title(\"C. Distribution of dCDF\")\n",
        "axs[2].legend()\n",
        "\n",
        "sns.ecdfplot(enzyme[\"dCDF\"].dropna(), ax=axs[3], label=\"Enzymatic Proteins\", color=color_palette[\"Enzyme\"])\n",
        "sns.ecdfplot(protein[\"dCDF\"].dropna(), ax=axs[3], label=\"Nonenzymatic Proteins\", color=color_palette[\"Protein\"])\n",
        "axs[3].set_title(\"D. Cumulative Distribution of dCDF\")\n",
        "axs[3].legend()\n",
        "\n",
        "sns.histplot(enzyme[\"PER(MDP)\"], kde=True, stat=\"density\", ax=axs[4], color=color_palette[\"Enzyme\"], label=\"Enzymatic Proteins\", alpha=0.6)\n",
        "sns.histplot(protein[\"PER(MDP)\"], kde=True, stat=\"density\", ax=axs[4], color=color_palette[\"Protein\"], label=\"Nonenzymatic Proteins\", alpha=0.6)\n",
        "axs[4].set_title(\"E. Distribution of PER(MDP)\")\n",
        "axs[4].legend()\n",
        "\n",
        "sns.ecdfplot(enzyme[\"dCH\"].dropna(), ax=axs[5], label=\"Enzymatic Proteins\", color=color_palette[\"Enzyme\"])\n",
        "sns.ecdfplot(protein[\"dCH\"].dropna(), ax=axs[5], label=\"Nonenzymatic Proteins\", color=color_palette[\"Protein\"])\n",
        "axs[5].set_title(\"F. Cumulative Distribution of dCH\")\n",
        "axs[5].legend()\n",
        "\n",
        "sns.ecdfplot(enzyme[\"PER(MDP)\"].dropna(), ax=axs[6], label=\"Enzymatic Proteins\", color=color_palette[\"Enzyme\"])\n",
        "sns.ecdfplot(protein[\"PER(MDP)\"].dropna(), ax=axs[6], label=\"Nonenzymatic Proteins\", color=color_palette[\"Protein\"])\n",
        "axs[6].set_title(\"G. Cumulative Distribution of PER(MDP)\")\n",
        "axs[6].legend()\n",
        "\n",
        "axs[7].axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(tiff_image_path, dpi=300, format=\"png\")\n",
        "plt.close()\n",
        "\n",
        "(summary_stats_path, ttest_results_path, tiff_image_path)\n"
      ],
      "metadata": {
        "id": "sSj4IiIRbs2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Code for generating QQ plots of untransformmed and log-transformed enzyme datasets"
      ],
      "metadata": {
        "id": "JCHLp0SO0dGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-import necessary modules after kernel reset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "# File paths\n",
        "file_paths = {\n",
        "    \"Non_Water_Multireactions\": \"/content/Processed_Cleaned_Non_Water_Multireactions_with_active_site_distances_with_EC.csv\",\n",
        "    \"Non_Water\": \"/content/Processed_Cleaned_Non_Water_with_active_site_distances_with_EC.csv\",\n",
        "    \"Water_Enzymes\": \"/content/Processed_Cleaned_Water_Enzymes_with_active_site_distances_with_EC.csv\",\n",
        "    \"Water_Multireactions\": \"/content/Processed_Cleaned_Water_Multireactions_with_active_site_distances_with_EC.csv\"\n",
        "}\n",
        "\n",
        "# Load and merge datasets\n",
        "combined_df = pd.DataFrame()\n",
        "for name, path in file_paths.items():\n",
        "    df = pd.read_csv(path)\n",
        "    df[\"Dataset\"] = name\n",
        "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "\n",
        "# Columns to analyze\n",
        "columns = [\"enzyme_sequence_length\", \"actual_count\", \"Average_Distance_to_Active_Site\", \"Tertiary_Distance\"]\n",
        "\n",
        "# Log transform columns\n",
        "for col in columns:\n",
        "    combined_df[f\"log_{col}\"] = np.log1p(combined_df[col])\n",
        "\n",
        "# Create QQ plots (before and after transformation)\n",
        "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(14, 18))\n",
        "fig.subplots_adjust(hspace=0.4, wspace=0.3)\n",
        "\n",
        "for i, col in enumerate(columns):\n",
        "    # Raw data QQ plot\n",
        "    stats.probplot(combined_df[col].dropna(), dist=\"norm\", plot=axes[i, 0])\n",
        "    axes[i, 0].get_lines()[0].set_markerfacecolor(\"steelblue\")\n",
        "    axes[i, 0].get_lines()[0].set_markeredgecolor(\"steelblue\")\n",
        "    axes[i, 0].get_lines()[1].set_color(\"black\")\n",
        "    axes[i, 0].set_title(f\"QQ Plot (Original) - {col}\", fontsize=14)\n",
        "    axes[i, 0].set_xlabel(\"Theoretical Quantiles\", fontsize=11)\n",
        "    axes[i, 0].set_ylabel(\"Sample Quantiles\", fontsize=11)\n",
        "\n",
        "    # Log-transformed data QQ plot\n",
        "    stats.probplot(combined_df[f\"log_{col}\"].dropna(), dist=\"norm\", plot=axes[i, 1])\n",
        "    axes[i, 1].get_lines()[0].set_markerfacecolor(\"firebrick\")\n",
        "    axes[i, 1].get_lines()[0].set_markeredgecolor(\"firebrick\")\n",
        "    axes[i, 1].get_lines()[1].set_color(\"black\")\n",
        "    axes[i, 1].set_title(f\"QQ Plot (Log-Transformed) - {col}\", fontsize=14)\n",
        "    axes[i, 1].set_xlabel(\"Theoretical Quantiles\", fontsize=11)\n",
        "    axes[i, 1].set_ylabel(\"Sample Quantiles\", fontsize=11)\n",
        "\n",
        "# Save the figure\n",
        "qq_plot_path = \"/content/QQ_Plots_Before_After_Log_Transformation.tiff\"\n",
        "plt.tight_layout()\n",
        "plt.savefig(qq_plot_path, dpi=300)\n",
        "plt.show()\n",
        "\n",
        "qq_plot_path\n"
      ],
      "metadata": {
        "id": "HSTIFfd80dKW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}